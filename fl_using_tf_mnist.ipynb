{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing \n",
    "def load(paths):\n",
    "    '''expects images for each class in seperate dir, \n",
    "        e.g all digits in 0 class in the directory named 0 and so on.'''\n",
    "    data = list()\n",
    "    labels = list()\n",
    "    # loop over the input images\n",
    "    for (i, imgpath) in enumerate(paths):\n",
    "        # load the image and extract the class labels. Since jpeg images so cv2 will work on it\n",
    "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
    "        image = np.array(im_gray).flatten()\n",
    "        label = imgpath.split(os.path.sep)[-2]\n",
    "        # scale the image to [0, 1] and add to list\n",
    "        data.append(image/255)\n",
    "        labels.append(label)\n",
    "        \n",
    "       \n",
    "    # return a tuple of the data and labels\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the path list using the path object\n",
    "img_path = 'trainingSet'\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply our load function\n",
    "image_list, label_list = load(image_paths)\n",
    "#image_list\n",
    "#label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "# since it is a multiclass classification so this will create a one-hot vector(one vesus all)\n",
    "lb = LabelBinarizer()\n",
    "#transforms multicalss labels to binary labels\n",
    "label_list = lb.fit_transform(label_list)\n",
    "#label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split data into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients(image_list, label_list, num_clients, initial='clients'):\n",
    "    ''' return: a dictionary with keys clients' names and value as \n",
    "                data shards - tuple of images and label lists.\n",
    "        args: \n",
    "            image_list: a list of numpy arrays of training images\n",
    "            label_list:a list of binarized labels for each image\n",
    "            num_client: number of fedrated members (clients)\n",
    "            initials: the clients'name prefix, e.g, clients_1 \n",
    "            \n",
    "    '''\n",
    "\n",
    "    #create a list of client names\n",
    "    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n",
    "\n",
    "    # zip() with 2 arguments,the function will return an iterator\n",
    "    # that generates tuples of length 2. then we make a list of tuples.\n",
    "    data = list(zip(image_list, label_list))\n",
    "    #randomize the data.\n",
    "    random.shuffle(data)\n",
    "\n",
    "    #shard data and place at each client\n",
    "    size = len(data)//num_clients\n",
    "    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    #number of clients must equal number of shards\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i] : shards[i] for i in range(len(client_names))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create clients\n",
    "clients = create_clients(X_train, y_train, num_clients=21, initial='clients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds(tensorflow dataset) object \n",
    "    '''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = Sequential()\n",
    "        model.add(Flatten(input_shape=(shape,)))\n",
    "        #model.add(Activation(\"sigmoid\"))\n",
    "        #model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(200, activation=\"relu\"))\n",
    "        #model.add(Activation(\"sigmoid\"))\n",
    "        #model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(classes, activation=\"softmax\"))\n",
    "        #model.add(Activation(\"sigmoid\"))\n",
    "        #model.add(Activation(\"softmax\"))\n",
    "        # since MNIST is an exclusive classiication dataset so softmax activation function for output\n",
    "        # layer and ReLU for hidden layers will work best \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 200)               157000    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 159,010\n",
      "Trainable params: 159,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SimpleMLP.build(784, 10)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Flatten at 0x22e9f434cc8>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x22ea73d2888>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x22e8f091ac8>]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMomentum is an argument in SGD optimizer which we could tweak to obtain faster convergence.\\nUnlike classical SGD, momentum method helps the parameter vector to build up velocity in any \\ndirection with constant gradient descent so as to prevent oscillations. A typical choice of \\nmomentum is between 0.5 to 0.9\\n'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we are using time-based decay here. it will reduce after every epoch by decay factor\n",
    "# if we use the more number of epochs then step decay will be best suited \n",
    "lr = 0.02 #learning rate\n",
    "update_round = 10\n",
    "decay_rate = lr/update_round\n",
    "loss='categorical_crossentropy' # since classification task\n",
    "metrics = ['accuracy']\n",
    "optimizer = SGD(lr=lr, decay=decay_rate, momentum=0.8, nesterov=False)\n",
    "\n",
    "'''\n",
    "Momentum is an argument in SGD optimizer which we could tweak to obtain faster convergence.\n",
    "Unlike classical SGD, momentum method helps the parameter vector to build up velocity in any \n",
    "direction with constant gradient descent so as to prevent oscillations. A typical choice of \n",
    "momentum is between 0.5 to 0.9\n",
    "'''\n",
    "#from tensorflow.keras.optimizers import Adadelta\n",
    "#optimizer = Adadelta(lr=lr, rho=0.95, epsilon=1e-08, decay=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " now we are estimating the weight parameters for each client based on the loss values recorded across every data point they trained with. On the left, we scaled each of those parameters and sum them all component-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(X_test, Y_test,  model, update_round):\n",
    "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "    logits = model.predict(X_test)\n",
    "    loss = cce(Y_test, logits)\n",
    "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
    "    print('update_round: {} | global_acc: {:.5%} | global_loss: {:.6%}'.format(update_round, acc, loss))\n",
    "    return acc, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 2ms/step - loss: 1.7103 - accuracy: 0.4749\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.7756 - accuracy: 0.4630\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8087 - accuracy: 0.4421\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8328 - accuracy: 0.4176\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8630 - accuracy: 0.4040\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8575 - accuracy: 0.4121\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8795 - accuracy: 0.4316\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.8843 - accuracy: 0.4204\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9367 - accuracy: 0.4094\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9437 - accuracy: 0.3854\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9516 - accuracy: 0.3977\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9907 - accuracy: 0.3639\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9741 - accuracy: 0.3881\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9939 - accuracy: 0.3608\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 1.9681 - accuracy: 0.3915\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0324 - accuracy: 0.3564\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0106 - accuracy: 0.3588\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0004 - accuracy: 0.3800\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0321 - accuracy: 0.3700\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0343 - accuracy: 0.3355\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 2.0324 - accuracy: 0.3364\n",
      "update_round: 0 | global_acc: 81.95238% | global_loss: 196.386909%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9071 - accuracy: 0.7976\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8948 - accuracy: 0.8258\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8878 - accuracy: 0.8263\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9153 - accuracy: 0.8032\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8677 - accuracy: 0.8302\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9202 - accuracy: 0.8024\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8628 - accuracy: 0.8342\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8983 - accuracy: 0.8245\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8944 - accuracy: 0.8202\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8988 - accuracy: 0.8081\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8636 - accuracy: 0.8294\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8816 - accuracy: 0.8114\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8591 - accuracy: 0.8519\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9297 - accuracy: 0.8181\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8959 - accuracy: 0.8335\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8872 - accuracy: 0.8189\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9116 - accuracy: 0.8197\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9331 - accuracy: 0.8017\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9021 - accuracy: 0.8232\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.8915 - accuracy: 0.8138\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.9072 - accuracy: 0.8162\n",
      "update_round: 1 | global_acc: 84.50000% | global_loss: 187.096465%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7538 - accuracy: 0.8416\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7516 - accuracy: 0.8317\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7280 - accuracy: 0.8284\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7215 - accuracy: 0.8524\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 0.7658 - accuracy: 0.8298\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7078 - accuracy: 0.8560\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7327 - accuracy: 0.8559\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7314 - accuracy: 0.8378\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6997 - accuracy: 0.8592\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7327 - accuracy: 0.8383\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6960 - accuracy: 0.8523\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7357 - accuracy: 0.8332\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7266 - accuracy: 0.8471\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7138 - accuracy: 0.8598\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7189 - accuracy: 0.8430\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7027 - accuracy: 0.8488\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7196 - accuracy: 0.8401\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7237 - accuracy: 0.8461\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7411 - accuracy: 0.8430\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7578 - accuracy: 0.8233\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.7431 - accuracy: 0.8500\n",
      "update_round: 2 | global_acc: 85.53571% | global_loss: 183.153093%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6727 - accuracy: 0.8425\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6874 - accuracy: 0.8391\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6416 - accuracy: 0.8513\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6511 - accuracy: 0.8603\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6481 - accuracy: 0.8538\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 0.6651 - accuracy: 0.8603\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6595 - accuracy: 0.8557\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.8696\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6678 - accuracy: 0.8530\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6310 - accuracy: 0.8735\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6845 - accuracy: 0.8367\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6508 - accuracy: 0.8540\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6715 - accuracy: 0.8605\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.8726\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6539 - accuracy: 0.8517\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6696 - accuracy: 0.8445\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6760 - accuracy: 0.8418\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.8371\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6994 - accuracy: 0.8406\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6704 - accuracy: 0.8497\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6328 - accuracy: 0.8635\n",
      "update_round: 3 | global_acc: 85.91667% | global_loss: 180.788112%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6349 - accuracy: 0.8493\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6056 - accuracy: 0.8689\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6507 - accuracy: 0.8478\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5982 - accuracy: 0.8674\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6176 - accuracy: 0.8717\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5678 - accuracy: 0.8851\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6426 - accuracy: 0.8516\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6191 - accuracy: 0.8500\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 0.6610 - accuracy: 0.8448\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6246 - accuracy: 0.8500\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6348 - accuracy: 0.8456\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6118 - accuracy: 0.8687\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6067 - accuracy: 0.8710\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6448 - accuracy: 0.8461\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.8585\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6437 - accuracy: 0.8544\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.8733\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6238 - accuracy: 0.8460\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.8608\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6184 - accuracy: 0.8422\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6157 - accuracy: 0.8533\n",
      "update_round: 4 | global_acc: 86.27381% | global_loss: 179.238534%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.8627\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.8690\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6337 - accuracy: 0.8507\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6300 - accuracy: 0.8521\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5908 - accuracy: 0.8605\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.8537\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.8566\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6099 - accuracy: 0.8442\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5768 - accuracy: 0.8692\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5404 - accuracy: 0.8840\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5825 - accuracy: 0.8766\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5987 - accuracy: 0.8629\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6405 - accuracy: 0.8456\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5721 - accuracy: 0.8680\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6435 - accuracy: 0.8372\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.8808\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6102 - accuracy: 0.8472\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5722 - accuracy: 0.8710\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5991 - accuracy: 0.8670\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6092 - accuracy: 0.8727\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5599 - accuracy: 0.8744\n",
      "update_round: 5 | global_acc: 86.59524% | global_loss: 178.091073%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5595 - accuracy: 0.8886\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5821 - accuracy: 0.8749\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5544 - accuracy: 0.8720\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5268 - accuracy: 0.8921\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5415 - accuracy: 0.8736\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6135 - accuracy: 0.8482\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6156 - accuracy: 0.8459\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5889 - accuracy: 0.8642\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5539 - accuracy: 0.8696\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5771 - accuracy: 0.8682\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 0.5617 - accuracy: 0.8561\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6083 - accuracy: 0.8492\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6006 - accuracy: 0.8534\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5168 - accuracy: 0.8893\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5790 - accuracy: 0.8604\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5567 - accuracy: 0.8797\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5823 - accuracy: 0.8526\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5791 - accuracy: 0.8692\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5887 - accuracy: 0.8552\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6230 - accuracy: 0.8535\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5945 - accuracy: 0.8496\n",
      "update_round: 6 | global_acc: 86.79762% | global_loss: 177.194309%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.8717\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6004 - accuracy: 0.8547\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5854 - accuracy: 0.8502\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5343 - accuracy: 0.8876\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5825 - accuracy: 0.8402\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5594 - accuracy: 0.8728\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5417 - accuracy: 0.8746\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5797 - accuracy: 0.8673\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5955 - accuracy: 0.8550\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.8687\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5597 - accuracy: 0.8630\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5881 - accuracy: 0.8563\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.8905\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5734 - accuracy: 0.8645\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 0.5175 - accuracy: 0.8970\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.8463\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5414 - accuracy: 0.8742\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5979 - accuracy: 0.8484\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5965 - accuracy: 0.8655\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.8715\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5686 - accuracy: 0.8521\n",
      "update_round: 7 | global_acc: 87.01190% | global_loss: 176.484501%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5776 - accuracy: 0.8537\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5707 - accuracy: 0.8530\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5313 - accuracy: 0.8606\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.8763\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.8920\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.8525\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5205 - accuracy: 0.8821\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5269 - accuracy: 0.8857\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5713 - accuracy: 0.8583\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5468 - accuracy: 0.8875\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5828 - accuracy: 0.8551\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5724 - accuracy: 0.8669\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4994 - accuracy: 0.8876\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5278 - accuracy: 0.8873\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.8656\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5352 - accuracy: 0.8814\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5737 - accuracy: 0.8553\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5822 - accuracy: 0.8543\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5640 - accuracy: 0.8474\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5753 - accuracy: 0.8557\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5718 - accuracy: 0.8508\n",
      "update_round: 8 | global_acc: 87.19048% | global_loss: 175.898123%\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5644 - accuracy: 0.8606\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.8591\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.8835\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.8787\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5173 - accuracy: 0.8788\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5553 - accuracy: 0.8743\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.8866\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5953 - accuracy: 0.8561\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5466 - accuracy: 0.8637\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.8898\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5316 - accuracy: 0.8641\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5346 - accuracy: 0.8694\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5744 - accuracy: 0.8554\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4870 - accuracy: 0.8936\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.8854\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.8542\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5494 - accuracy: 0.8576\n",
      "50/50 [==============================] - 1s 2ms/step - loss: 0.5553 - accuracy: 0.8664\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5174 - accuracy: 0.8780\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5371 - accuracy: 0.8728\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5407 - accuracy: 0.8656\n",
      "update_round: 9 | global_acc: 87.26190% | global_loss: 175.401413%\n"
     ]
    }
   ],
   "source": [
    "#initialize global model: using multi-layer-perceptron model\n",
    "smlp_global = SimpleMLP()\n",
    "# .build method automatically builds the MLP model on our data\n",
    "global_model = smlp_global.build(784, 10)\n",
    "\n",
    "#commence global training loop\n",
    "for update_round in range(update_round):\n",
    "            \n",
    "    # weight intialisation\n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        # this creates the mlp model for the clients. \n",
    "        # we can also create deiffenrt model for each client\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "                      \n",
    "                      \n",
    "        #set local model weight to the weight of the global model that acts as the initial update \n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, update_round)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8726190476190476, <tf.Tensor: shape=(), dtype=float32, numpy=1.7540141>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_acc, global_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now comparing it with SGD mehtod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 2.2291 - accuracy: 0.1779\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.8073 - accuracy: 0.5874\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.4886 - accuracy: 0.7121\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.2500 - accuracy: 0.7560\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.0689 - accuracy: 0.7875\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 0.9518 - accuracy: 0.8025\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.8596 - accuracy: 0.8156\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 0.7867 - accuracy: 0.8277\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 9ms/step - loss: 0.7315 - accuracy: 0.8354: 0s - loss: 0.7369 - ac\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 8ms/step - loss: 0.6770 - accuracy: 0.8474\n",
      "update_round: 1 | global_acc: 85.11905% | global_loss: 181.986022%\n"
     ]
    }
   ],
   "source": [
    "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
    "smlp_SGD = SimpleMLP()\n",
    "SGD_model = smlp_SGD.build(784, 10) \n",
    "\n",
    "SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# fit the SGD training data to model\n",
    "history = SGD_model.fit(SGD_dataset, epochs=10)\n",
    "\n",
    "#test the SGD global model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "       SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the above learning curve we can see that the loss is decreasing, so we increase the number of epochs. For better understanding on fitting the data to model we can create a validation set that will also tell us the case of overfitting and underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## considering non-IID data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_iid_x(image_list, label_list, x=1, num_intraclass_clients=21):\n",
    "        ''' creates x non_IID clients\n",
    "        args: \n",
    "            image_list: python list of images or data points\n",
    "            label_list: python list of labels\n",
    "            x: none IID severity, 1 means each client will only have one class of data\n",
    "            num_intraclass_client: number of sub-client to be created from each none IID class,\n",
    "            e.g for x=1, we could create 10 further clients by splitting each class into 10\n",
    "            \n",
    "        return - dictionary \n",
    "            keys - clients's name, \n",
    "            value - client's non iid 1 data shard (as tuple list of images and labels) '''\n",
    "        \n",
    "        non_iid_x_clients = dict()\n",
    "        \n",
    "        #create unique label list and shuffle\n",
    "        unique_labels = np.unique(np.array(label_list))\n",
    "        random.shuffle(unique_labels)\n",
    "        \n",
    "        #create sub label lists based on x\n",
    "        sub_lab_list = [unique_labels[i:i + x] for i in range(0, len(unique_labels), x)]\n",
    "            \n",
    "        for item in sub_lab_list:\n",
    "            class_data = [(image, label) for (image, label) in zip(image_list, label_list) if label in item]\n",
    "            \n",
    "            #decouple tuple list into seperate image and label lists\n",
    "            images, labels = zip(*class_data)\n",
    "            \n",
    "            # create formated client initials\n",
    "            initial = ''\n",
    "            for lab in item:\n",
    "                initial = initial + str(lab) + '_'\n",
    "            \n",
    "            #create num_intraclass_clients clients from the class \n",
    "            intraclass_clients = create_clients(list(images), list(labels), num_intraclass_clients, initial)\n",
    "            \n",
    "            #append intraclass clients to main clients'dict\n",
    "            non_iid_x_clients.update(intraclass_clients)\n",
    "        \n",
    "        return non_iid_x_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_iid_clients = non_iid_x(image_list, label_list, 1, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, bs=32):\n",
    "    '''Takes in a clients data shard and create a tfds object off it\n",
    "    args:\n",
    "        shard: a data, label constituting a client's data shard\n",
    "        bs:batch size\n",
    "    return:\n",
    "        tfds(tensorflow dataset) object \n",
    "    '''\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process and batch the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in non_iid_clients.items():\n",
    "    clients_batched[client_name] = batch_data(data)\n",
    "    \n",
    "#process and batch the test set  \n",
    "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_scalling_factor(clients_trn_data, client_name):\n",
    "    client_names = list(clients_trn_data.keys())\n",
    "    #get the bs\n",
    "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
    "    #first calculate the total training data points across clinets\n",
    "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n",
    "    # get the total number of data points held by a client\n",
    "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n",
    "    return local_count/global_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weight, scalar):\n",
    "    '''function for scaling a models weights'''\n",
    "    weight_final = []\n",
    "    steps = len(weight)\n",
    "    for i in range(steps):\n",
    "        weight_final.append(scalar * weight[i])\n",
    "    return weight_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(scaled_weight_list):\n",
    "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
    "    avg_grad = list()\n",
    "    #get the average grad accross all client gradients\n",
    "    for grad_list_tuple in zip(*scaled_weight_list):\n",
    "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
    "        avg_grad.append(layer_mean)\n",
    "        \n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 1ms/step - loss: 2.2984 - accuracy: 0.1352\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3165 - accuracy: 0.1294\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3106 - accuracy: 0.1317\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3145 - accuracy: 0.1275\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3077 - accuracy: 0.1467\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3164 - accuracy: 0.1369\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3183 - accuracy: 0.1395\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3076 - accuracy: 0.1470\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3284 - accuracy: 0.1295\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3479 - accuracy: 0.1253\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3044 - accuracy: 0.1458\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.2971 - accuracy: 0.1460\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3166 - accuracy: 0.1453\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3159 - accuracy: 0.1290\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3120 - accuracy: 0.1479\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3072 - accuracy: 0.1456\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3274 - accuracy: 0.1344\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3110 - accuracy: 0.1393\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3083 - accuracy: 0.1404\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3286 - accuracy: 0.1380\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3200 - accuracy: 0.1400\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3226 - accuracy: 0.1345\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3098 - accuracy: 0.1445\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3328 - accuracy: 0.1326\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 2.3487 - accuracy: 0.1210\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3013 - accuracy: 0.1425\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3073 - accuracy: 0.1432\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3200 - accuracy: 0.1426\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3208 - accuracy: 0.1406\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3267 - accuracy: 0.1196\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3279 - accuracy: 0.1383\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3324 - accuracy: 0.1338\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.2966 - accuracy: 0.1523\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3298 - accuracy: 0.1316\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3505 - accuracy: 0.1269\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3375 - accuracy: 0.1434\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3293 - accuracy: 0.1362\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3118 - accuracy: 0.1429\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3117 - accuracy: 0.1440\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.3428 - accuracy: 0.1299\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3211 - accuracy: 0.1353\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.3178 - accuracy: 0.1306\n",
      "update_round: 0 | global_acc: 25.44048% | global_loss: 227.867889%\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1093 - accuracy: 0.2710\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 2.1088 - accuracy: 0.2860\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0960 - accuracy: 0.2948\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0960 - accuracy: 0.2968\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1013 - accuracy: 0.2926\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0919 - accuracy: 0.2879\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0939 - accuracy: 0.3149\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1137 - accuracy: 0.2899\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1077 - accuracy: 0.2669\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.0987 - accuracy: 0.2873\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 2.1109 - accuracy: 0.2850\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0957 - accuracy: 0.3010\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1105 - accuracy: 0.2805\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1042 - accuracy: 0.2832\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1289 - accuracy: 0.2553\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1223 - accuracy: 0.2794\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1209 - accuracy: 0.2772\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.0963 - accuracy: 0.2884\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 2.1236 - accuracy: 0.2735\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1095 - accuracy: 0.2946\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1180 - accuracy: 0.2864\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1144 - accuracy: 0.2829\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1040 - accuracy: 0.2856\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1265 - accuracy: 0.2776\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0964 - accuracy: 0.2966\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1457 - accuracy: 0.2499\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1117 - accuracy: 0.2880\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1177 - accuracy: 0.2770\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1150 - accuracy: 0.2810\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1006 - accuracy: 0.2881\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.0955 - accuracy: 0.3033\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1154 - accuracy: 0.2713\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1237 - accuracy: 0.2805\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1099 - accuracy: 0.2959\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1044 - accuracy: 0.2969\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1083 - accuracy: 0.2985\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.0982 - accuracy: 0.2889\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1098 - accuracy: 0.2695\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1350 - accuracy: 0.2640\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 2.1202 - accuracy: 0.2720\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 2.1137 - accuracy: 0.2666\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 2.1088 - accuracy: 0.2764\n",
      "update_round: 1 | global_acc: 39.36905% | global_loss: 225.814366%\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9645 - accuracy: 0.4310\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9740 - accuracy: 0.4048\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9553 - accuracy: 0.4382\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9705 - accuracy: 0.4242\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9629 - accuracy: 0.4294\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9706 - accuracy: 0.3985\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9591 - accuracy: 0.4269\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9623 - accuracy: 0.4065\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9534 - accuracy: 0.4526\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9540 - accuracy: 0.4380\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9581 - accuracy: 0.4316\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9667 - accuracy: 0.4199\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9626 - accuracy: 0.4235\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9529 - accuracy: 0.4674\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9479 - accuracy: 0.4406\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9515 - accuracy: 0.4307\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9505 - accuracy: 0.4477\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9519 - accuracy: 0.4411\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9798 - accuracy: 0.4072\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9453 - accuracy: 0.4379\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 1.9641 - accuracy: 0.4309\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9587 - accuracy: 0.4198\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9633 - accuracy: 0.4204\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9544 - accuracy: 0.4274\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9804 - accuracy: 0.4105\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9868 - accuracy: 0.4166\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9897 - accuracy: 0.4049\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9761 - accuracy: 0.4189\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9742 - accuracy: 0.4170\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9756 - accuracy: 0.4220\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9593 - accuracy: 0.4268\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9367 - accuracy: 0.4554\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9675 - accuracy: 0.4183\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.9679 - accuracy: 0.4337\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9870 - accuracy: 0.4067\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9527 - accuracy: 0.4213\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9686 - accuracy: 0.4078\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9674 - accuracy: 0.4208\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9660 - accuracy: 0.4127\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9550 - accuracy: 0.4301\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9620 - accuracy: 0.4183\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.9732 - accuracy: 0.4132\n",
      "update_round: 2 | global_acc: 52.39286% | global_loss: 223.873949%\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.8593 - accuracy: 0.5363\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8288 - accuracy: 0.5642\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8678 - accuracy: 0.5253\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8532 - accuracy: 0.5356\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8530 - accuracy: 0.5085\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8568 - accuracy: 0.5556\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8455 - accuracy: 0.5491\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8419 - accuracy: 0.5380\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8609 - accuracy: 0.5543\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8627 - accuracy: 0.5315\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8616 - accuracy: 0.5348\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8561 - accuracy: 0.5365\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8331 - accuracy: 0.5611\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8455 - accuracy: 0.5391\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8725 - accuracy: 0.5053\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8686 - accuracy: 0.5253\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8494 - accuracy: 0.5528\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8482 - accuracy: 0.5407\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8456 - accuracy: 0.5419\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8343 - accuracy: 0.5648\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8415 - accuracy: 0.5735\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8353 - accuracy: 0.5556\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.8424 - accuracy: 0.5414\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8508 - accuracy: 0.5576\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8518 - accuracy: 0.5328\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8472 - accuracy: 0.5526\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8451 - accuracy: 0.5520\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8535 - accuracy: 0.5375\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8450 - accuracy: 0.5564\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8456 - accuracy: 0.5345\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8571 - accuracy: 0.5426\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8417 - accuracy: 0.5422\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8350 - accuracy: 0.5508\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8522 - accuracy: 0.5505\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.8748 - accuracy: 0.5258\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8492 - accuracy: 0.5417\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8469 - accuracy: 0.5416\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8520 - accuracy: 0.5526\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8372 - accuracy: 0.5300\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8456 - accuracy: 0.5648\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8698 - accuracy: 0.5312\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.8537 - accuracy: 0.5418\n",
      "update_round: 3 | global_acc: 61.27381% | global_loss: 222.046256%\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7497 - accuracy: 0.6204\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7433 - accuracy: 0.6412\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.7590 - accuracy: 0.6083\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7612 - accuracy: 0.6085\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7705 - accuracy: 0.6113\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7322 - accuracy: 0.6550\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7353 - accuracy: 0.6273\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7493 - accuracy: 0.6196\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7553 - accuracy: 0.6289\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7468 - accuracy: 0.6322\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7533 - accuracy: 0.6242\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7562 - accuracy: 0.6102\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7535 - accuracy: 0.6338\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7386 - accuracy: 0.6235\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7620 - accuracy: 0.6136\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7557 - accuracy: 0.6227\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7506 - accuracy: 0.6170\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7430 - accuracy: 0.6295\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7593 - accuracy: 0.6270\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7533 - accuracy: 0.6281\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7477 - accuracy: 0.5992\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7550 - accuracy: 0.6185\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7404 - accuracy: 0.6176\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7601 - accuracy: 0.5903\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.7335 - accuracy: 0.6295\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7513 - accuracy: 0.6196\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7402 - accuracy: 0.6412\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7421 - accuracy: 0.6530\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7645 - accuracy: 0.6269\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7846 - accuracy: 0.5913\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7678 - accuracy: 0.6194\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7384 - accuracy: 0.6309\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7791 - accuracy: 0.6141\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7604 - accuracy: 0.6002\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7571 - accuracy: 0.6307\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7460 - accuracy: 0.6434\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7739 - accuracy: 0.5954\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7639 - accuracy: 0.6018\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7557 - accuracy: 0.6344\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.7461 - accuracy: 0.6138\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7565 - accuracy: 0.6189\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.7534 - accuracy: 0.6104\n",
      "update_round: 4 | global_acc: 66.17857% | global_loss: 220.336509%\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6791 - accuracy: 0.6537\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6544 - accuracy: 0.6760\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6662 - accuracy: 0.6598\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6684 - accuracy: 0.6722\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 1.6974 - accuracy: 0.6548\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6714 - accuracy: 0.6399\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6861 - accuracy: 0.6488\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6740 - accuracy: 0.6743\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6897 - accuracy: 0.6561\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6647 - accuracy: 0.6727\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6747 - accuracy: 0.6732\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6724 - accuracy: 0.6663\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6666 - accuracy: 0.6678\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6828 - accuracy: 0.6632\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6845 - accuracy: 0.6621\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6703 - accuracy: 0.6961\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6808 - accuracy: 0.6404\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6829 - accuracy: 0.6289\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6514 - accuracy: 0.6830\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6700 - accuracy: 0.6672\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6875 - accuracy: 0.6658\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6739 - accuracy: 0.6757\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6835 - accuracy: 0.6619\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6924 - accuracy: 0.6599\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6902 - accuracy: 0.6649\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6568 - accuracy: 0.6760\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6588 - accuracy: 0.6821\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 1.6941 - accuracy: 0.6506\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6732 - accuracy: 0.6760\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6502 - accuracy: 0.6724\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6713 - accuracy: 0.6809\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6565 - accuracy: 0.6792\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6796 - accuracy: 0.6693\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6898 - accuracy: 0.6597\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6574 - accuracy: 0.6705\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6815 - accuracy: 0.6742\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6809 - accuracy: 0.6652\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6701 - accuracy: 0.6721\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6918 - accuracy: 0.6639\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6773 - accuracy: 0.6576\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6632 - accuracy: 0.6729\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6930 - accuracy: 0.6390\n",
      "update_round: 5 | global_acc: 68.96429% | global_loss: 218.738818%\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5863 - accuracy: 0.7021\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6121 - accuracy: 0.6840\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6196 - accuracy: 0.6768\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5938 - accuracy: 0.7037\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6187 - accuracy: 0.6933\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5908 - accuracy: 0.6972\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6305 - accuracy: 0.6677\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 1.6076 - accuracy: 0.6820\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6031 - accuracy: 0.7080\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5944 - accuracy: 0.6977\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6147 - accuracy: 0.6886\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6223 - accuracy: 0.6806\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6071 - accuracy: 0.6976\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6036 - accuracy: 0.7028\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5888 - accuracy: 0.6938\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6165 - accuracy: 0.6864\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5909 - accuracy: 0.6962\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.6135 - accuracy: 0.7033\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6304 - accuracy: 0.6891\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6055 - accuracy: 0.7084\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6014 - accuracy: 0.7030\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5904 - accuracy: 0.7078\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6010 - accuracy: 0.7103\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6083 - accuracy: 0.7127\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5949 - accuracy: 0.6961\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6039 - accuracy: 0.6911\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6210 - accuracy: 0.6948\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6056 - accuracy: 0.7073\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6032 - accuracy: 0.7218\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.6044 - accuracy: 0.6913\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6147 - accuracy: 0.6911\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5933 - accuracy: 0.7146\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6084 - accuracy: 0.7033\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6038 - accuracy: 0.6819\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6284 - accuracy: 0.6722\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5925 - accuracy: 0.7017\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5925 - accuracy: 0.6936\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5884 - accuracy: 0.7161\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6129 - accuracy: 0.6949\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5943 - accuracy: 0.7037\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6027 - accuracy: 0.7069\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.6060 - accuracy: 0.6974\n",
      "update_round: 6 | global_acc: 71.19048% | global_loss: 217.250967%\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5316 - accuracy: 0.7355\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5488 - accuracy: 0.7191\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5409 - accuracy: 0.7127\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5140 - accuracy: 0.7291\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5440 - accuracy: 0.7200\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5438 - accuracy: 0.6951\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5729 - accuracy: 0.6965\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5525 - accuracy: 0.7073\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5389 - accuracy: 0.7177\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 1.5610 - accuracy: 0.7025\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5318 - accuracy: 0.7320\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5387 - accuracy: 0.7353\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5378 - accuracy: 0.7127\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5583 - accuracy: 0.7163\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5299 - accuracy: 0.7121\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5540 - accuracy: 0.7083\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5510 - accuracy: 0.7183\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5351 - accuracy: 0.7150\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5410 - accuracy: 0.7180\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5277 - accuracy: 0.7140\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5224 - accuracy: 0.7203\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5338 - accuracy: 0.7019\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5530 - accuracy: 0.7261\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5373 - accuracy: 0.7049\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5361 - accuracy: 0.7153\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5767 - accuracy: 0.6981\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5625 - accuracy: 0.7021\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5517 - accuracy: 0.7142\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5479 - accuracy: 0.7190\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5405 - accuracy: 0.7224\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5558 - accuracy: 0.7103\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5382 - accuracy: 0.7121\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 1.5652 - accuracy: 0.6918\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5854 - accuracy: 0.6951\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5246 - accuracy: 0.7160\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5261 - accuracy: 0.7308\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5521 - accuracy: 0.7133\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5344 - accuracy: 0.7328\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5579 - accuracy: 0.7099\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5576 - accuracy: 0.6990\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5592 - accuracy: 0.6830\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5462 - accuracy: 0.7022\n",
      "update_round: 7 | global_acc: 72.86905% | global_loss: 215.861058%\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5111 - accuracy: 0.7087\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4964 - accuracy: 0.7221\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5114 - accuracy: 0.7284\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5221 - accuracy: 0.7073\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4908 - accuracy: 0.7335\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4926 - accuracy: 0.7247\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4960 - accuracy: 0.7287\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4931 - accuracy: 0.7237\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4783 - accuracy: 0.7434\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4768 - accuracy: 0.7477\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.4888 - accuracy: 0.7496\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4813 - accuracy: 0.7330\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4916 - accuracy: 0.7361\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4772 - accuracy: 0.7207\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4931 - accuracy: 0.7243\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4769 - accuracy: 0.7361\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5002 - accuracy: 0.7073\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4864 - accuracy: 0.7315\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4587 - accuracy: 0.7629\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4906 - accuracy: 0.6957\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4958 - accuracy: 0.7133\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4793 - accuracy: 0.7362\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5031 - accuracy: 0.7289\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4778 - accuracy: 0.7462\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4889 - accuracy: 0.7349\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5060 - accuracy: 0.7271\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4624 - accuracy: 0.7374\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4735 - accuracy: 0.7401\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4927 - accuracy: 0.7318\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4927 - accuracy: 0.7096\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4705 - accuracy: 0.7584\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4644 - accuracy: 0.7395\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4653 - accuracy: 0.7239\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4792 - accuracy: 0.7371\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4871 - accuracy: 0.7244\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4926 - accuracy: 0.7446\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4836 - accuracy: 0.7345\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4870 - accuracy: 0.7408\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.4866 - accuracy: 0.7336\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.5005 - accuracy: 0.7333\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 1.5067 - accuracy: 0.6993\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 1.4713 - accuracy: 0.7279\n",
      "update_round: 8 | global_acc: 74.27381% | global_loss: 214.566684%\n"
     ]
    }
   ],
   "source": [
    "#initialize global model: using multi-layer-perceptron model\n",
    "smlp_global = SimpleMLP()\n",
    "# .build method automatically builds the MLP model on our data\n",
    "global_model = smlp_global.build(784, 10)\n",
    "        \n",
    "#commence global training loop\n",
    "for update_round in range(update_round):\n",
    "            \n",
    "    # weight intialisation\n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "    \n",
    "    #initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    #randomize client data - using keys\n",
    "    client_names= list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    \n",
    "    #loop through each client and create new local model\n",
    "    for client in client_names:\n",
    "        # this creates the mlp model for the clients. \n",
    "        # we can also create deiffenrt model for each client\n",
    "        smlp_local = SimpleMLP()\n",
    "        local_model = smlp_local.build(784, 10)\n",
    "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "                      \n",
    "                      \n",
    "        #set local model weight to the weight of the global model that acts as the initial update \n",
    "        local_model.set_weights(global_weights)\n",
    "        \n",
    "        #fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=1)\n",
    "        \n",
    "        #scale the model weights and add to list\n",
    "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "        \n",
    "        #clear session to free memory after each communication round\n",
    "        K.clear_session()\n",
    "        \n",
    "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "    \n",
    "    #update global model \n",
    "    global_model.set_weights(average_weights)\n",
    "\n",
    "    #test global model and print out metrics after each communications round\n",
    "    for(X_test, Y_test) in test_batched:\n",
    "        global_acc, global_loss = test_model(X_test, Y_test, global_model, update_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 2.3564 - accuracy: 0.1189\n",
      "Epoch 2/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 2.1952 - accuracy: 0.2435\n",
      "Epoch 3/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 2.0634 - accuracy: 0.3707\n",
      "Epoch 4/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.9490 - accuracy: 0.4685\n",
      "Epoch 5/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.8413 - accuracy: 0.5511\n",
      "Epoch 6/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 1.7430 - accuracy: 0.6101\n",
      "Epoch 7/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.6480 - accuracy: 0.6559\n",
      "Epoch 8/10\n",
      "105/105 [==============================] - 1s 7ms/step - loss: 1.5603 - accuracy: 0.6865\n",
      "Epoch 9/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 1.4820 - accuracy: 0.7095\n",
      "Epoch 10/10\n",
      "105/105 [==============================] - 1s 6ms/step - loss: 1.4083 - accuracy: 0.7255\n",
      "update_round: 1 | global_acc: 73.63095% | global_loss: 211.357236%\n"
     ]
    }
   ],
   "source": [
    "SGD_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(y_train)).batch(320)\n",
    "smlp_SGD = SimpleMLP()\n",
    "SGD_model = smlp_SGD.build(784, 10) \n",
    "\n",
    "SGD_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "# fit the SGD training data to model\n",
    "history = SGD_model.fit(SGD_dataset, epochs=10)\n",
    "\n",
    "#test the SGD global model and print out metrics\n",
    "for(X_test, Y_test) in test_batched:\n",
    "       SGD_acc, SGD_loss = test_model(X_test, Y_test, SGD_model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 6))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this setup, we can see that non_iid_data learns very slow with respect to iid_data but the positive point is that accuracy is increasing and loss is decreasing.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
